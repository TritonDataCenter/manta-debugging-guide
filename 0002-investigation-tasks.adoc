== Basic investigation tasks

=== Investigating a slow tier of Node processes

The following services within the Manta data path are Node.js-based:

- Webapi (muskie)
- Electric-Moray
- Moray
- Authcache (mahi)
- Nameservice (not covered in this section)

NOTE: This section assumes that you're looking at a specific tier of services
(i.e., one of the ones listed above).  **If you're looking at Moray
specifically, as you follow this section, consider only the Moray instances for
the particular shard you're looking at.**

First, **have you already confirmed that this tier is reporting high latency?**
If not, check metric dashboards first to see whether latency is high.  See
<<_investigating_an_increase_in_latency>>.

Next, **have you also confirmed that the reason for the latency is not due to a
dependent service?**  For example, if you're here because of Electric-Moray,
have you confirmed that Moray isn't also seeing high latency?  If you haven't,
see <<_investigating_an_increase_in_latency>> for how to locate which tier is
the origin of overall latency.

At this point, you've confirmed that this tier appears to be the source of a
latency increase.  Now, use the latency metric dashboards to see **is the
increase in latency affecting nearly all instances?**

* **Yes, nearly all instances are reporting high latency.**  This might reflect
  insufficient capacity.  Individual Node processes are usually close to
  capacity when they exceed about 80-85% of CPU utilization.  Each component
  typically has multiple processes deployed in each zone, which means the
  saturation point (80-85% per process) is different for each type of zone:
+
--
[cols="<,>,>,>"]
|===
<| Service      <| Processes per zone <| Saturation point  <| Equivalent CPUs

| webapi         | 16                  | 13000% (zone-wide) | 13 CPUs
| electric-moray |  4                  | 325% (zone-wide)   | 3.25 CPUs
| moray          |  4                  | 325% (zone-wide)   | 3.25 CPUs
| authcache      |  1                  |  80% (zone-wide)   | 0.8 CPUs
|===

Given the above guides, use CPU utilization graphs to determine: **are most
instances in this service close to CPU saturation?**  (The
<<_deployment_specific_details>> section recommends having both a line graph
showing the minimum zone-wide CPU usage for each service and a heat map of
zone-wide CPU utilization.  If the line graph is above these guidelines, the
service definitely looks saturated.  The heat map allows you to identify cases
where some instances might have more headroom available, but most of them are
still too busy.)
--
** **Yes, most instances are nearing saturation.**  Deploy more instances of the
    service in question.  See <<_scaling_up_a_component>>.  **Additionally, if
    the workload has not changed substantially, you may want to
    <<_check_for_a_garbage_collection_issue_or_memory_leak, check for a memory
    leak>> that may have affected many processes.**  If it has, then deploying
    more instances will likely only help for a little while -- until those
    suffer the same leak.
** **No, many instances appear to have plenty of headroom.**  This is very
   unusual, so it's worth double-checking that latency is elevated across the
   board, but latency at dependent services is not high, and CPU utilization is
   not high.  If this is really the case, pick any specific process showing high
   latency and see <<_investigating_a_slow_process>>.  Other avenues to
   consider: **is the dependent service close to CPU saturation?** If so,
   clients of the dependent service may see much higher latency than the service
   reports because of queueing.  **Is there evidence of elevated packet loss?**
   This can also increase client-side latency without manifesting as latency
   reported by the dependent service.
* **No, only some instances are reporting high latency.**  In this case, this
  service does not appear generally overloaded, although it's possible that some
  instances are.  Next question: **can you tell from the per-request metrics for
  this tier whether the workload is evenly distributed across instances?**
** **Yes, the workload is evenly distributed across instances.**  In this case,
  it appears that processes are generally doing comparable work, but some are
  doing it much slower.  The next step is to **use the dashboard to identify a
  zone with particularly high latency and dig deeper into a specific slow
  process**.  See <<_investigating_a_slow_process>>.
** **No, the workload is unevenly distributed.**  See
  <<_investigating_service_discovery>>.  (Remember, if you're looking at Moray,
  this section assumes you're looking only at instances for a particular shard.
  As described under <<_investigating_an_increase_in_latency>>, if you see
  latency at Moray, you should first isolate the shard and investigate latency
  in each shard separately.)


=== Investigating PostgreSQL latency

=== Finding (or generating) a failed request

When trying to understand either an explicit error or high latency, it can be
helpful to investigate the log entry written by "webapi" for a specific request.

**Do you already have information about a specific request you want to
investigate?**

* **Yes, I have information about a specific request.** See
  <<_investigating_a_specific_request_that_has_failed>>.
* **No, I don't have information about a specific request yet.**  Move on.

Next question: **does the problem appear to be reproducible?**  Try reproducing
the problem with the https://github.com/joyent/node-manta[node-manta]
command-line tools (e.g., `mls`).  You can use the `-v` flag and redirect stderr
to the https://github.com/trentm/node-bunyan[bunyan] command to see request and
response headers, like this:

[source,text]
----
$ mls -v /poseidon/public 2> >(bunyan)
[2018-07-19T21:18:48.146Z] DEBUG: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/lib/client.js:1536 in ls): ls: entered (req_id=4b4927be-fc1f-4dd8-88fe-2ae75dcbc262, path=/poseidon/public)
[2018-07-19T21:18:48.148Z] DEBUG: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/lib/client.js:1128 in info): info: entered (req_id=0f34c68e-2072-405a-be3a-248e8020f1ba, path=/poseidon/public, id=0f34c68e-2072-405a-be3a-248e8020f1ba, query={})
    headers: {
      "accept": "application/json, */*",
      "x-request-id": "0f34c68e-2072-405a-be3a-248e8020f1ba"
    }
[2018-07-19T21:18:48.189Z] TRACE: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/node_modules/restify-clients/lib/HttpClient.js:314 in rawRequest): request sent
    HEAD /poseidon/public HTTP/1.1
    Host: us-east.manta.joyent.com:null
    accept: application/json, */*
    x-request-id: 0f34c68e-2072-405a-be3a-248e8020f1ba
    date: Thu, 19 Jul 2018 21:18:48 GMT
    authorization: Signature keyId="/dap/keys/56:f3:e1:56:3d:e6:f7:83:a9:ce:19:5d:62:ba:5c:1f",algorithm="rsa-sha1",headers="date",signature="kG7IydhNO06ImfI6hFzFXXoSrWT6+2kCcDUC3swGebIr7YxeDcLEWMxGzB4z5lC29Vv7kgpLGaTc218m+63D0Y3M84LTNCvM1Va5COetXhIHkkAlBtXpJt5MUjqsRFK1xrpGKJjDc1QIBGSQIDmh4p6wNjofeaLX8jYnYa7FagW5iyQIHQmpAwe/AO+9Bg7fXBgzfvVZjWfhLaBA4G2CwuCSlkpF7mR7t04pTn+oxOmufE5h6XI/VLNsLZyQkc6prBFDoSiOLMgZsGfdsF11J9c/lCK/PW1y4MlTZBDGG8W1F0ssUEx0euLdm4TsqoBc1cfeIC43fV6sR2nN/CSiow=="
    user-agent: restify/1.4.1 (x64-darwin; v8/4.5.103.53; OpenSSL/1.0.2o) node/4.9.1
    accept-version: ~1.0
[2018-07-19T21:18:48.861Z] TRACE: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/node_modules/restify-clients/lib/HttpClient.js:210 in onResponse): Response received
    HTTP/1.1 200 OK
    content-type: application/x-json-stream; type=directory
    result-set-size: 5
    date: Thu, 19 Jul 2018 21:18:48 GMT
    server: Manta
    x-request-id: 0f34c68e-2072-405a-be3a-248e8020f1ba
    x-response-time: 254
    x-server-name: 511ea59e-2a4a-486b-9258-ef59016d064d
    connection: keep-alive
    x-request-received: 1532035128176
    x-request-processing-time: 684
[2018-07-19T21:18:48.867Z] DEBUG: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/lib/client.js:820 in get): get: entered (req_id=dce478bd-6bc7-451b-ac2b-22d74d7bfd37, path=/poseidon/public, id=dce478bd-6bc7-451b-ac2b-22d74d7bfd37)
    headers: {
      "accept": "application/x-json-stream",
      "x-request-id": "4b4927be-fc1f-4dd8-88fe-2ae75dcbc262",
      "date": "Thu, 19 Jul 2018 21:18:48 GMT",
      "authorization": "Signature keyId=\"/dap/keys/56:f3:e1:56:3d:e6:f7:83:a9:ce:19:5d:62:ba:5c:1f\",algorithm=\"rsa-sha1\",headers=\"date\",signature=\"kG7IydhNO06ImfI6hFzFXXoSrWT6+2kCcDUC3swGebIr7YxeDcLEWMxGzB4z5lC29Vv7kgpLGaTc218m+63D0Y3M84LTNCvM1Va5COetXhIHkkAlBtXpJt5MUjqsRFK1xrpGKJjDc1QIBGSQIDmh4p6wNjofeaLX8jYnYa7FagW5iyQIHQmpAwe/AO+9Bg7fXBgzfvVZjWfhLaBA4G2CwuCSlkpF7mR7t04pTn+oxOmufE5h6XI/VLNsLZyQkc6prBFDoSiOLMgZsGfdsF11J9c/lCK/PW1y4MlTZBDGG8W1F0ssUEx0euLdm4TsqoBc1cfeIC43fV6sR2nN/CSiow==\""
    }
    --
    query: {
      "limit": 1024
    }
[2018-07-19T21:18:48.872Z] TRACE: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/node_modules/restify-clients/lib/HttpClient.js:314 in rawRequest): request sent
    GET /poseidon/public?limit=1024 HTTP/1.1
    Host: us-east.manta.joyent.com:null
    accept: application/x-json-stream
    x-request-id: 4b4927be-fc1f-4dd8-88fe-2ae75dcbc262
    date: Thu, 19 Jul 2018 21:18:48 GMT
    authorization: Signature keyId="/dap/keys/56:f3:e1:56:3d:e6:f7:83:a9:ce:19:5d:62:ba:5c:1f",algorithm="rsa-sha1",headers="date",signature="kG7IydhNO06ImfI6hFzFXXoSrWT6+2kCcDUC3swGebIr7YxeDcLEWMxGzB4z5lC29Vv7kgpLGaTc218m+63D0Y3M84LTNCvM1Va5COetXhIHkkAlBtXpJt5MUjqsRFK1xrpGKJjDc1QIBGSQIDmh4p6wNjofeaLX8jYnYa7FagW5iyQIHQmpAwe/AO+9Bg7fXBgzfvVZjWfhLaBA4G2CwuCSlkpF7mR7t04pTn+oxOmufE5h6XI/VLNsLZyQkc6prBFDoSiOLMgZsGfdsF11J9c/lCK/PW1y4MlTZBDGG8W1F0ssUEx0euLdm4TsqoBc1cfeIC43fV6sR2nN/CSiow=="
    user-agent: restify/1.4.1 (x64-darwin; v8/4.5.103.53; OpenSSL/1.0.2o) node/4.9.1
    accept-version: ~1.0
[2018-07-19T21:18:49.365Z] TRACE: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/node_modules/restify-clients/lib/HttpClient.js:210 in onResponse): Response received
    HTTP/1.1 200 OK
    content-type: application/x-json-stream; type=directory
    result-set-size: 5
    date: Thu, 19 Jul 2018 21:18:49 GMT
    server: Manta
    x-request-id: 4b4927be-fc1f-4dd8-88fe-2ae75dcbc262
    x-response-time: 219
    x-server-name: 60771e58-2ad0-4c50-8b23-86b72f9307f8
    connection: keep-alive
    transfer-encoding: chunked
    x-request-received: 1532035128869
    x-request-processing-time: 496
-rwxr-xr-x 1 poseidon            17 Dec 04  2015 agent.sh
drwxr-xr-x 1 poseidon             0 Sep 18  2014 manatee
drwxr-xr-x 1 poseidon             0 Jun 18  2013 medusa
drwxr-xr-x 1 poseidon             0 Aug 01  2013 minke
drwxr-xr-x 1 poseidon             0 Nov 07  2013 stud
[2018-07-19T21:18:49.480Z] DEBUG: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/lib/client.js:887 in onEnd): get: done (req_id=dce478bd-6bc7-451b-ac2b-22d74d7bfd37, path=/poseidon/public)
----

From the output, we can see that this operation made two requests.  The second
one has `x-server-name: 60771e58-2ad0-4c50-8b23-86b72f9307f8` and `x-request-id:
4b4927be-fc1f-4dd8-88fe-2ae75dcbc262`.  You can now use these to start
<<_investigating_a_specific_request_that_has_failed>>.

**If the problem isn't quite so easily reproducible, but you suspect it still
affects a variety of requests,** you can use
https://github.com/joyent/manta-mlive[manta-mlive] to generate more requests and
collect debug output for all of them.  For `mlive`, you'll want to set
`LOG_LEVEL` in the environment to generate the debug logging and you'll likely
want to redirect stderr to a file that you can search through later:

[source,text]
----
$ LOG_LEVEL=trace ./bin/mlive -S 2>debug.out
2018-07-19T21:24:01.307Z: reads okay, writes stuck (4/4 ok since start)
2018-07-19T21:24:02.310Z: all okay (17/17 ok since 2018-07-19T21:24:01.307Z)
^C
----

As before, you can use the `bunyan` tool to format the log:

[source,text]
----
$ bunyan debug.out
...
[2018-07-19T21:25:01.716Z] TRACE: mlive/MantaClient/9435 on zathras.local: request sent
    HEAD /dap/stor/mlive HTTP/1.1
    Host: us-east.manta.joyent.com:null
    accept: application/json, */*
    x-request-id: c317603c-82d4-4b2e-ac4b-066c9ece1864
    date: Thu, 19 Jul 2018 21:25:01 GMT
    authorization: Signature keyId="/dap/keys/56:f3:e1:56:3d:e6:f7:83:a9:ce:19:5d:62:ba:5c:1f",algorithm="rsa-sha1",headers="date",signature="oJZZIDh1qT8PeSSpz09bIzYT4LYK6rqXS2G5bHhh2r37SNOs0vBkFHUhfso6tSq1hmHIlkCEMXX9zGLIvYxQtHj6/KtiNgZgyWzGHms+qhc2gziXnOrMybxmWqJwipd8rAJCdDBV0B5FlCDeELWIA+1LifGDqqLdDZT4ScBUNOm9JG2+mha2U+pFbNtaXQQyyoPgopk+4ur4OHYpcaK/KY6WdC91quLIaIKV28VMtPoN/q/15lzRj6G6L7mbIMyd48ut0EbmTTR/CfYq9dquNsWDlyWgEJJVYyPZ9odAE34YQiYt/N4JXH7Crr9M6md9GtZonY+DbP8vvb5+7xr8dA=="
    user-agent: restify/1.4.1 (x64-darwin; v8/4.5.103.53; OpenSSL/1.0.2o) node/4.9.1
    accept-version: ~1.0
[2018-07-19T21:25:02.548Z] TRACE: mlive/MantaClient/9435 on zathras.local: Response received
    HTTP/1.1 200 OK
    last-modified: Tue, 16 Dec 2014 01:17:29 GMT
    content-type: application/x-json-stream; type=directory
    result-set-size: 45
    date: Thu, 19 Jul 2018 21:25:02 GMT
    server: Manta
    x-request-id: c317603c-82d4-4b2e-ac4b-066c9ece1864
    x-response-time: 462
    x-server-name: 39adec6c-bded-4a14-9d80-5a8bfc1121f9
    connection: keep-alive
    x-request-received: 1532035501703
    x-request-processing-time: 844
----

You can use `grep`, `json`, or other tools to filter the output for requests of
interest (e.g., those with a particular HTTP response code or an
`x-response-time` larger than some value).  From the filtered results, you can
identify an `x-server-name` and `x-request-id` and then see
<<_investigating_a_specific_request_that_has_failed>>.




=== Checking recent historical metrics

// This section should mostly reference another section that describes deploying
// Prometheus, Grafana, etc, as well as what dashboards to have, and what
// metrics they should provide.
// error rate, muskie tail latency, moray queue depth, moray tail latency

=== Locate a specific zone

=== Log into a specific zone

=== Understanding a Muskie log entry

XXX talk about common stack traces?
XXX that should include 503 from 'No storage nodes available for this request'

=== Understanding latency for a specific request

=== Finding a load balancer log entry

=== Understanding a load balancer log entry

=== Build a request timeline

// XXX talk about how to include client, load balancer, muskie, metadata, mako
// details

=== Details about specific error messages

==== `No storage nodes available for this request`

==== `Not enough free space for ... MB`

This code (associated with 507 errors) indicates that Manta does not have enough
space available on any storage nodes for the write _that was requested_.  This
would be surprising in production environments, although it's easy to induce
even in production by requesting an absurd size.  For example, you'll see this
if you attempt to upload an enormous object:

[source,text]
----
$ mput -H 'max-content-length: 1125899906842624' /dap/stor/waytoobig
mput: NotEnoughSpaceError: not enough free space for 1073741824 MB
----

https://apidocs.joyent.com/manta/api.html#PutObject[Recall that Manta supports
two kinds of uploads]: streaming and fixed-length.

Streaming uploads are specified using the `transfer-encoding: chunked` header.
In this case, the space allocated up front (and validated) is specified by the
`max-content-length` header.  If that header is missing, a default value is used
_that is sometimes too large in some development environments_, which can cause
streaming uploads to fail in development environments.

If an object PUT does not specify `transfer-encoding: chunked`, then it must
specify `content-length`, in which case that value is used for allocation (and
validation).

=== Locating metadata for an object

See http://joyent.github.io/manta/#locating-object-data[Locating object data] in
the Manta Operator's Guide.

=== Locating actual data for an object

See http://joyent.github.io/manta/#locating-object-data[Locating object data] in
the Manta Operator's Guide.

=== Locating a particular server

See http://joyent.github.io/manta/#locating-servers[Locating servers] in the
Manta Operator's Guide.

=== Locating a particular zone

See http://joyent.github.io/manta/#locating-manta-component-zones[Locating Manta
component zones] in the Manta Operator's Guide.

=== Locating a particular database shard


=== Finding what shard a particular zone is part of


=== Save debugging state and restart a process

// TODO include filing a bug

=== Investigating service discovery

// TODO what's in DNS

=== Investigating a slow process

// TODO this should probably just be: check if it's on-CPU and branch to one of
// the following steps

=== Investigating why a process is on-CPU

// TODO profiling a busy process

=== Investigating why a process is off-CPU

=== Check for a garbage collection issue (or memory leak)

//    - Check if the process is on-CPU
//    - Checking if a process is doing GC
//    - Check the vsz/rss
//    - Gcore and restart
//    - Watch for a few minutes.  Is it better?
//      Yes: check other instances for vsz/rss and restart them
//      No: maybe leak is too fast?
//      - Was there a recent change that may have introduced this?
//        Yes: rollback
//        No: debug memory leak


=== Scaling up a component

// TODO: caveats for webapi (will restart load balancers), moray (will want to
// tune *down* database connections, or be aware that you'll be using more)


== Characterizing a problem

It's very valuable to _briefly_ and _precisely_ characterize a problem:

- when asking for help from others, so they can quickly understand the relevant
  context
- for summarizing status to stakeholders
- for handing off work to others (e.g., when escalating to another team or
  changing shifts)
- as a form of
  https://en.wikipedia.org/wiki/Rubber_duck_debugging[rubber-ducking] -- it
  often helps spark insight

When characterizing a problem, include the basic facts:

- Exactly **what** you've observed: 500 errors? 503 errors? Elevated latency?
  It's useful to be as specific as you can (e.g., "a 5% increase in 500
  errors"), but it's better to be vague (e.g., "an increase in latency") than to
  say something false (e.g., "an increase in average latency" when only tail
  latency is affected).
- Something about **when** the observations started.  Again, it's useful to be
  as specific as possible (e.g., "starting at 2018-08-09T16:47Z"), but it's
  better to be vague (e.g., "seems to have increased since yesterday") than
  incorrect (e.g., "it's higher than yesterday" when you're really just
  eyeballing an average value from a graph).
- What other observations you've made (or anything else you've tried, and the
  results)
- Any specific questions you have.

It's useful to mention what conclusions you've drawn, but try to distinguish
facts (e.g., "moray queue lengths are high") from inferences ("moray is
overloaded").

Here's a good characterization:

[quote]
Webapi is reporting an increase in 500 and 503 errors since 2018-08-09T16:58Z.
These do not seem correlated with increased latency, nor with any errors from
the metadata tier.  How do we check for errors from the storage tier?

It's often very helpful to include screenshots
(https://support.apple.com/en-us/HT201361[MacOS instructions]).  Here's an
example:

image::images/metrics-webapi-elevated-1.png[Elevated latency at Webapi]

Here's one way to characterize this:

[quote]
We saw elevated latency since early on 8/1 (UTC)

Better:

[quote]
We saw a modest increase in tail latency (both p90 and p99) starting early on
8/1 and continuing through 8/4 (UTC).  This was followed by a sharp, significant
increase on 8/6.

More detailed (may be better, depending on the situation):

[quote]
We saw a modest increase in tail latency (both p90 and p99) starting early on
8/1 and continuing through 8/4 (UTC).  This was followed by a sharp, significant
increase on 8/6.  During both increases, latency was much less stable than
before.  p90 peaked at about 50% higher than normal, while p95 spiked about 3x
what it was before.

Of course, it's easy to give too much detail, too.  Think about what's likely
to be relevant for your audience.

**Grafana's features can help you make quantitative statements.**  For example,
take this graph:

image::images/metrics-webapi-bytes-1.png[Dips in throughput at Webapi]

About the dip on 8/6, you could say:

[quote]
We saw a dip in throughput on 8/6.

Better would be:

[quote]
There was a brief dip in inbound throughput on 8/6 around 12:36Z.

Since Grafana can show you both the average for the whole period, plus the value
at a point, you can make more specific statements:

[quote]
There was a brief dip in inbound throughput on 8/6 around 12:36Z.  It dropped
about 38% relative to the average for 14 days around it (3.3 GBps during the
dip, compared to a recent average of about 5.3 GBps).
