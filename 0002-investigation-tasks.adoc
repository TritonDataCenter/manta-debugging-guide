== Basic investigation tasks

=== Investigating a slow tier of Node processes

The following services within the Manta data path are Node.js-based:

- Webapi (muskie)
- Electric-Moray
- Moray
- Authcache (mahi)
- Nameservice (not covered in this section)

NOTE: This section assumes that you're looking at a specific tier of services
(i.e., one of the ones listed above).  **If you're looking at Moray
specifically, as you follow this section, consider only the Moray instances for
the particular shard you're looking at.**

First, **have you already confirmed that this tier is reporting high latency?**
If not, check metric dashboards first to see whether latency is high.  See
<<_investigating_an_increase_in_latency>>.

Next, **have you also confirmed that the reason for the latency is not due to a
dependent service?**  For example, if you're here because of Electric-Moray,
have you confirmed that Moray isn't also seeing high latency?  If you haven't,
see <<_investigating_an_increase_in_latency>> for how to locate which tier is
the origin of overall latency.

At this point, you've confirmed that this tier appears to be the source of a
latency increase.  Now, use the latency metric dashboards to see **is the
increase in latency affecting nearly all instances?**

* **Yes, nearly all instances are reporting high latency.**  This might reflect
  insufficient capacity.  Individual Node processes are usually close to
  capacity when they exceed about 80-85% of CPU utilization.  Each component
  typically has multiple processes deployed in each zone, which means the
  saturation point (80-85% per process) is different for each type of zone:
+
--
[cols="<,>,>,>"]
|===
<| Service      <| Processes per zone <| Saturation point  <| Equivalent CPUs

| webapi         | 16                  | 13000% (zone-wide) | 13 CPUs
| electric-moray |  4                  | 325% (zone-wide)   | 3.25 CPUs
| moray          |  4                  | 325% (zone-wide)   | 3.25 CPUs
| authcache      |  1                  |  80% (zone-wide)   | 0.8 CPUs
|===

Given the above guides, use CPU utilization graphs to determine: **are most
instances in this service close to CPU saturation?**  (The
<<_deployment_specific_details>> section recommends having both a line graph
showing the minimum zone-wide CPU usage for each service and a heat map of
zone-wide CPU utilization.  If the line graph is above these guidelines, the
service definitely looks saturated.  The heat map allows you to identify cases
where some instances might have more headroom available, but most of them are
still too busy.)
--
** **Yes, most instances are nearing saturation.**  Deploy more instances of the
    service in question.  See <<_scaling_up_a_component>>.  **Additionally, if
    the workload has not changed substantially, you may want to
    <<_check_for_a_garbage_collection_issue_or_memory_leak, check for a memory
    leak>> that may have affected many processes.**  If it has, then deploying
    more instances will likely only help for a little while -- until those
    suffer the same leak.
** **No, many instances appear to have plenty of headroom.**  This is very
   unusual, so it's worth double-checking that latency is elevated across the
   board, but latency at dependent services is not high, and CPU utilization is
   not high.  If this is really the case, pick any specific process showing high
   latency and see <<_investigating_a_slow_process>>.  Other avenues to
   consider: **is the dependent service close to CPU saturation?** If so,
   clients of the dependent service may see much higher latency than the service
   reports because of queueing.  **Is there evidence of elevated packet loss?**
   This can also increase client-side latency without manifesting as latency
   reported by the dependent service.
* **No, only some instances are reporting high latency.**  In this case, this
  service does not appear generally overloaded, although it's possible that some
  instances are.  Next question: **can you tell from the per-request metrics for
  this tier whether the workload is evenly distributed across instances?**
** **Yes, the workload is evenly distributed across instances.**  In this case,
  it appears that processes are generally doing comparable work, but some are
  doing it much slower.  The next step is to **use the dashboard to identify a
  zone with particularly high latency and dig deeper into a specific slow
  process**.  See <<_investigating_a_slow_process>>.
** **No, the workload is unevenly distributed.**  See
  <<_investigating_service_discovery>>.  (Remember, if you're looking at Moray,
  this section assumes you're looking only at instances for a particular shard.
  As described under <<_investigating_an_increase_in_latency>>, if you see
  latency at Moray, you should first isolate the shard and investigate latency
  in each shard separately.)


=== Investigating PostgreSQL latency

=== Finding (or generating) a failed request

When trying to understand either an explicit error or high latency, it can be
helpful to investigate the log entry written by "webapi" for a specific request.

**Do you already have information about a specific request you want to
investigate?**

* **Yes, I have information about a specific request.** See
  <<_investigating_a_specific_request_that_has_failed>>.
* **No, I don't have information about a specific request yet.**  Move on.

Next question: **does the problem appear to be reproducible?**  Try reproducing
the problem with the https://github.com/joyent/node-manta[node-manta]
command-line tools (e.g., `mls`).  You can use the `-v` flag and redirect stderr
to the https://github.com/trentm/node-bunyan[bunyan] command to see request and
response headers, like this:

[source,text]
----
$ mls -v /poseidon/public 2> >(bunyan)
[2018-07-19T21:18:48.146Z] DEBUG: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/lib/client.js:1536 in ls): ls: entered (req_id=4b4927be-fc1f-4dd8-88fe-2ae75dcbc262, path=/poseidon/public)
[2018-07-19T21:18:48.148Z] DEBUG: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/lib/client.js:1128 in info): info: entered (req_id=0f34c68e-2072-405a-be3a-248e8020f1ba, path=/poseidon/public, id=0f34c68e-2072-405a-be3a-248e8020f1ba, query={})
    headers: {
      "accept": "application/json, */*",
      "x-request-id": "0f34c68e-2072-405a-be3a-248e8020f1ba"
    }
[2018-07-19T21:18:48.189Z] TRACE: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/node_modules/restify-clients/lib/HttpClient.js:314 in rawRequest): request sent
    HEAD /poseidon/public HTTP/1.1
    Host: us-east.manta.joyent.com:null
    accept: application/json, */*
    x-request-id: 0f34c68e-2072-405a-be3a-248e8020f1ba
    date: Thu, 19 Jul 2018 21:18:48 GMT
    authorization: Signature keyId="/dap/keys/56:f3:e1:56:3d:e6:f7:83:a9:ce:19:5d:62:ba:5c:1f",algorithm="rsa-sha1",headers="date",signature="kG7IydhNO06ImfI6hFzFXXoSrWT6+2kCcDUC3swGebIr7YxeDcLEWMxGzB4z5lC29Vv7kgpLGaTc218m+63D0Y3M84LTNCvM1Va5COetXhIHkkAlBtXpJt5MUjqsRFK1xrpGKJjDc1QIBGSQIDmh4p6wNjofeaLX8jYnYa7FagW5iyQIHQmpAwe/AO+9Bg7fXBgzfvVZjWfhLaBA4G2CwuCSlkpF7mR7t04pTn+oxOmufE5h6XI/VLNsLZyQkc6prBFDoSiOLMgZsGfdsF11J9c/lCK/PW1y4MlTZBDGG8W1F0ssUEx0euLdm4TsqoBc1cfeIC43fV6sR2nN/CSiow=="
    user-agent: restify/1.4.1 (x64-darwin; v8/4.5.103.53; OpenSSL/1.0.2o) node/4.9.1
    accept-version: ~1.0
[2018-07-19T21:18:48.861Z] TRACE: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/node_modules/restify-clients/lib/HttpClient.js:210 in onResponse): Response received
    HTTP/1.1 200 OK
    content-type: application/x-json-stream; type=directory
    result-set-size: 5
    date: Thu, 19 Jul 2018 21:18:48 GMT
    server: Manta
    x-request-id: 0f34c68e-2072-405a-be3a-248e8020f1ba
    x-response-time: 254
    x-server-name: 511ea59e-2a4a-486b-9258-ef59016d064d
    connection: keep-alive
    x-request-received: 1532035128176
    x-request-processing-time: 684
[2018-07-19T21:18:48.867Z] DEBUG: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/lib/client.js:820 in get): get: entered (req_id=dce478bd-6bc7-451b-ac2b-22d74d7bfd37, path=/poseidon/public, id=dce478bd-6bc7-451b-ac2b-22d74d7bfd37)
    headers: {
      "accept": "application/x-json-stream",
      "x-request-id": "4b4927be-fc1f-4dd8-88fe-2ae75dcbc262",
      "date": "Thu, 19 Jul 2018 21:18:48 GMT",
      "authorization": "Signature keyId=\"/dap/keys/56:f3:e1:56:3d:e6:f7:83:a9:ce:19:5d:62:ba:5c:1f\",algorithm=\"rsa-sha1\",headers=\"date\",signature=\"kG7IydhNO06ImfI6hFzFXXoSrWT6+2kCcDUC3swGebIr7YxeDcLEWMxGzB4z5lC29Vv7kgpLGaTc218m+63D0Y3M84LTNCvM1Va5COetXhIHkkAlBtXpJt5MUjqsRFK1xrpGKJjDc1QIBGSQIDmh4p6wNjofeaLX8jYnYa7FagW5iyQIHQmpAwe/AO+9Bg7fXBgzfvVZjWfhLaBA4G2CwuCSlkpF7mR7t04pTn+oxOmufE5h6XI/VLNsLZyQkc6prBFDoSiOLMgZsGfdsF11J9c/lCK/PW1y4MlTZBDGG8W1F0ssUEx0euLdm4TsqoBc1cfeIC43fV6sR2nN/CSiow==\""
    }
    --
    query: {
      "limit": 1024
    }
[2018-07-19T21:18:48.872Z] TRACE: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/node_modules/restify-clients/lib/HttpClient.js:314 in rawRequest): request sent
    GET /poseidon/public?limit=1024 HTTP/1.1
    Host: us-east.manta.joyent.com:null
    accept: application/x-json-stream
    x-request-id: 4b4927be-fc1f-4dd8-88fe-2ae75dcbc262
    date: Thu, 19 Jul 2018 21:18:48 GMT
    authorization: Signature keyId="/dap/keys/56:f3:e1:56:3d:e6:f7:83:a9:ce:19:5d:62:ba:5c:1f",algorithm="rsa-sha1",headers="date",signature="kG7IydhNO06ImfI6hFzFXXoSrWT6+2kCcDUC3swGebIr7YxeDcLEWMxGzB4z5lC29Vv7kgpLGaTc218m+63D0Y3M84LTNCvM1Va5COetXhIHkkAlBtXpJt5MUjqsRFK1xrpGKJjDc1QIBGSQIDmh4p6wNjofeaLX8jYnYa7FagW5iyQIHQmpAwe/AO+9Bg7fXBgzfvVZjWfhLaBA4G2CwuCSlkpF7mR7t04pTn+oxOmufE5h6XI/VLNsLZyQkc6prBFDoSiOLMgZsGfdsF11J9c/lCK/PW1y4MlTZBDGG8W1F0ssUEx0euLdm4TsqoBc1cfeIC43fV6sR2nN/CSiow=="
    user-agent: restify/1.4.1 (x64-darwin; v8/4.5.103.53; OpenSSL/1.0.2o) node/4.9.1
    accept-version: ~1.0
[2018-07-19T21:18:49.365Z] TRACE: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/node_modules/restify-clients/lib/HttpClient.js:210 in onResponse): Response received
    HTTP/1.1 200 OK
    content-type: application/x-json-stream; type=directory
    result-set-size: 5
    date: Thu, 19 Jul 2018 21:18:49 GMT
    server: Manta
    x-request-id: 4b4927be-fc1f-4dd8-88fe-2ae75dcbc262
    x-response-time: 219
    x-server-name: 60771e58-2ad0-4c50-8b23-86b72f9307f8
    connection: keep-alive
    transfer-encoding: chunked
    x-request-received: 1532035128869
    x-request-processing-time: 496
-rwxr-xr-x 1 poseidon            17 Dec 04  2015 agent.sh
drwxr-xr-x 1 poseidon             0 Sep 18  2014 manatee
drwxr-xr-x 1 poseidon             0 Jun 18  2013 medusa
drwxr-xr-x 1 poseidon             0 Aug 01  2013 minke
drwxr-xr-x 1 poseidon             0 Nov 07  2013 stud
[2018-07-19T21:18:49.480Z] DEBUG: mls/MantaClient/7054 on zathras.local (/Users/dap/install/node-v4.9.1-darwin-x64/lib/node_modules/manta/lib/client.js:887 in onEnd): get: done (req_id=dce478bd-6bc7-451b-ac2b-22d74d7bfd37, path=/poseidon/public)
----

From the output, we can see that this operation made two requests.  The second
one has `x-server-name: 60771e58-2ad0-4c50-8b23-86b72f9307f8` and `x-request-id:
4b4927be-fc1f-4dd8-88fe-2ae75dcbc262`.  You can now use these to start
<<_investigating_a_specific_request_that_has_failed>>.

**If the problem isn't quite so easily reproducible, but you suspect it still
affects a variety of requests,** you can use
https://github.com/joyent/manta-mlive[manta-mlive] to generate more requests and
collect debug output for all of them.  For `mlive`, you'll want to set
`LOG_LEVEL` in the environment to generate the debug logging and you'll likely
want to redirect stderr to a file that you can search through later:

[source,text]
----
$ LOG_LEVEL=trace ./bin/mlive -S 2>debug.out
2018-07-19T21:24:01.307Z: reads okay, writes stuck (4/4 ok since start)
2018-07-19T21:24:02.310Z: all okay (17/17 ok since 2018-07-19T21:24:01.307Z)
^C
----

As before, you can use the `bunyan` tool to format the log:

[source,text]
----
$ bunyan debug.out
...
[2018-07-19T21:25:01.716Z] TRACE: mlive/MantaClient/9435 on zathras.local: request sent
    HEAD /dap/stor/mlive HTTP/1.1
    Host: us-east.manta.joyent.com:null
    accept: application/json, */*
    x-request-id: c317603c-82d4-4b2e-ac4b-066c9ece1864
    date: Thu, 19 Jul 2018 21:25:01 GMT
    authorization: Signature keyId="/dap/keys/56:f3:e1:56:3d:e6:f7:83:a9:ce:19:5d:62:ba:5c:1f",algorithm="rsa-sha1",headers="date",signature="oJZZIDh1qT8PeSSpz09bIzYT4LYK6rqXS2G5bHhh2r37SNOs0vBkFHUhfso6tSq1hmHIlkCEMXX9zGLIvYxQtHj6/KtiNgZgyWzGHms+qhc2gziXnOrMybxmWqJwipd8rAJCdDBV0B5FlCDeELWIA+1LifGDqqLdDZT4ScBUNOm9JG2+mha2U+pFbNtaXQQyyoPgopk+4ur4OHYpcaK/KY6WdC91quLIaIKV28VMtPoN/q/15lzRj6G6L7mbIMyd48ut0EbmTTR/CfYq9dquNsWDlyWgEJJVYyPZ9odAE34YQiYt/N4JXH7Crr9M6md9GtZonY+DbP8vvb5+7xr8dA=="
    user-agent: restify/1.4.1 (x64-darwin; v8/4.5.103.53; OpenSSL/1.0.2o) node/4.9.1
    accept-version: ~1.0
[2018-07-19T21:25:02.548Z] TRACE: mlive/MantaClient/9435 on zathras.local: Response received
    HTTP/1.1 200 OK
    last-modified: Tue, 16 Dec 2014 01:17:29 GMT
    content-type: application/x-json-stream; type=directory
    result-set-size: 45
    date: Thu, 19 Jul 2018 21:25:02 GMT
    server: Manta
    x-request-id: c317603c-82d4-4b2e-ac4b-066c9ece1864
    x-response-time: 462
    x-server-name: 39adec6c-bded-4a14-9d80-5a8bfc1121f9
    connection: keep-alive
    x-request-received: 1532035501703
    x-request-processing-time: 844
----

You can use `grep`, `json`, or other tools to filter the output for requests of
interest (e.g., those with a particular HTTP response code or an
`x-response-time` larger than some value).  From the filtered results, you can
identify an `x-server-name` and `x-request-id` and then see
<<_investigating_a_specific_request_that_has_failed>>.




=== Checking recent historical metrics

// This section should mostly reference another section that describes deploying
// Prometheus, Grafana, etc, as well as what dashboards to have, and what
// metrics they should provide.
// error rate, muskie tail latency, moray queue depth, moray tail latency

=== Locating log files

**Real-time logs** contain data from the current hour.  These are typically
stored as regular files within each zone.  The specific file used varies by type
of zone.  See the https://joyent.github.io/manta/#logs[Operator Guide] for
details.  The `manta-oneach` command can be used as a low-level way to scan
these real-time logs.  For example, a common way to count recent 500-level
errors in webapi logs is:

    manta-oneach --service=webapi 'grep -c "handled: 5" /var/log/muskie.log'

Since the real-time logs only store the current hour's data, at 01:02Z, this
would only scan 2 minutes worth of data.  At 01:58Z, this would scan 58 minutes
worth of data.

If we were looking for a specific request that took place at 01:37Z, then we'd
look at the real-time log file immediately after the request happened until
02:00Z (i.e., for 23 minutes).  After that, we'd have to look at the historical
logs.

**Historical logs** are maintained by rotating the real-time logs at the top of
each hour and then uploading them into Manta.  Once in Manta, they're stored
under:

    /poseidon/stor/logs/COMPONENT/YYYY/MM/DD/HH/SHORTZONE.log

where:

* `COMPONENT` varies based on the component you're looking for
* `YYYY`, `MM`, `DD`, and `HH` represent the year, month, day, and hour for the
  entries in the log file
* `SHORTZONE` is the first 8 characters of the zone's uuid.

For example, to find the load balancer (haproxy) logs from zone
f6817865-10fb-416c-a079-47941ac2aab4 from 2018-12-05T01:37Z, we would look for
the object:

    /poseidon/stor/logs/haproxy/2018/12/05/01/f6817865.log

You can scan a number of these logs at once using a compute job.  For example,
you could look for requests from IP address 10.1.2.3 in all of the load
balancer logs from December 5, 2018 using:

    mfind -t o /poseidon/stor/logs/haproxy/2018/12/05 |
        mjob create -o -m 'grep 10.1.2.3'

You can adjust the `mfind` invocation as needed to scan a broader or more narrow
range.  You could also use the `-n` argument to `mfind` to select log files from
a particular load balancer:

    mfind -n f6817865.log -t o /poseidon/stor/logs/haproxy/2018/12/05 |
        mjob create -o -m 'grep 10.1.2.3'

Instead of using a compute job, you can also download the files individually and
analyze them locally.

**Note:** the archival process for historical logs first rotates the logs to new
files under `/var/log/manta/upload`.  A few minutes later, these are uploaded to
Manta and then removed from the local filesystem.  If the upload fails, the
files are kept in `/var/log/manta/upload` for up to two days.  In extreme
situations where Manta has been down for over an hour, you may find recent
historical log files in `/var/log/manta/upload`, and you can scan them similar
to the live log files using `manta-oneach`.

=== Understanding a Muskie log entry

Muskie (the Manta API server) logs an _audit_ entry for every request that it
completes.  These logs are useful for:

- understanding how Manta handled a particular request (e.g., how long did it
  take?  where was the time spent?  what metadata shards were involved?  what
  storage nodes were involved?  what errors were encountered?)
- understanding the workload Manta is serving (e.g., what percentage of requests
  are GETs?  what percentage are failing?  what's the distribution of sizes for
  uploaded objects?  which accounts are making the most requests?)

Note that for real-time incident response, it's often faster to start with
<<_checking_recent_historical_metrics>>.  These logs are most useful for digging
deeper into a particular request or the workload overall.

Muskie logs in https://github.com/trentm/node-bunyan[bunyan] format, which is a
JSON-based format.  You typically use the `bunyan` tool to view them.  You can
also use the `json` tool to filter and aggregate them.

==== Contents of a GET log entry

Here's an example log entry for a **GET request**, formatted using the `bunyan`
command-line tool:

    [2017-08-01T03:03:13.985Z]  INFO: muskie/HttpServer/79465 on 204ac483-7e7e-4083-9ea2-c9ea22f459fd: handled: 200 (audit=true, _audit=true, operation=getstorage, billable_operation=LIST, logicalRemoteAddress=172.27.4.22, remoteAddress=127.0.0.1, remotePort=64628, reqHeaderLength=754, resHeaderLength=269, err=false, latency=26, entryShard=tcp://3.moray.staging.joyent.us:2020, route=getstorage, req.owner=4d649f41-cf87-ca1d-c2c0-bb6a9004311d)
        GET /poseidon/stor/manta_gc/mako/1.stor.staging.joyent.us?limit=1024 HTTP/1.1
        accept: */*
        x-request-id: a080d88b-8e42-4a98-a6ec-12e1b0dbf612
        date: Tue, 01 Aug 2017 03:03:13 GMT
        authorization: Signature keyId="/poseidon/keys/ef:0e:27:45:c5:95:4e:92:ba:ab:03:17:e5:3a:60:14",algorithm="rsa-sha256",headers="date",signature="Q74o9RHIwrDT15ogL2WeB/jankUIqJAtMM5t7+VzrHxzoB52/BoqEnq9uMY0wEvPJxv+Lf1VyLG/IBXCXeUx+fZlkhKWIWd2jkpLRdVLKwZ4nnqTfHM+YXhZ0vSN1X1W2demmnpPRTRK/RaG21pyvlbIrSTwI+N5MtKFDh9/4Ks43wSyM4MvqZZWywfs7LgKz7UtjL1Z+juhJDT8mrfQYCDpZw/NDhHmoslKsMFesMrMjPALy/CBSB23800+MhLiFB7LT0nTyCLonPBmIOjrQCZu99ICXbCxx096XCzZ2XBOK1Pe4eoDUHWx5ukTbCJV63QA+gvcvDCbS5BdDn0Xiw=="
        user-agent: restify/1.4.1 (ia32-sunos; v8/3.14.5.9; OpenSSL/1.0.1i) node/0.10.32
        accept-version: ~1.0
        host: manta.staging.joyent.us
        connection: keep-alive
        x-forwarded-for: ::ffff:172.27.4.22
        --
        HTTP/1.1 200 OK
        last-modified: Sat, 22 Mar 2014 01:17:01 GMT
        content-type: application/x-json-stream; type=directory
        result-set-size: 1
        date: Tue, 01 Aug 2017 03:03:13 GMT
        server: Manta
        x-request-id: a080d88b-8e42-4a98-a6ec-12e1b0dbf612
        x-response-time: 26
        x-server-name: 204ac483-7e7e-4083-9ea2-c9ea22f459fd
        --
        req.caller: {
          "login": "poseidon",
          "uuid": "4d649f41-cf87-ca1d-c2c0-bb6a9004311d",
          "groups": [
            "operators"
          ],
          "user": null
        }
        --
        req.timers: {
          "earlySetup": 32,
          "parseDate": 8,
          "parseQueryString": 28,
          "handler-3": 127,
          "checkIfPresigned": 3,
          "enforceSSL": 3,
          "ensureDependencies": 5,
          "_authSetup": 5,
          "preSignedUrl": 3,
          "checkAuthzScheme": 4,
          "parseAuthTokenHandler": 36,
          "signatureHandler": 73,
          "parseKeyId": 59,
          "loadCaller": 133,
          "verifySignature": 483,
          "parseHttpAuthToken": 5,
          "loadOwner": 268,
          "getActiveRoles": 43,
          "gatherContext": 27,
          "setup": 225,
          "getMetadata": 5790,
          "storageContext": 8,
          "authorize": 157,
          "ensureEntryExists": 3,
          "assertMetadata": 3,
          "getDirectoryCount": 7903,
          "getDirectory": 10245
        }

The raw JSON, formatted with the `json` tool, looks like this:

[source,json]
----
{
  "name": "muskie",
  "hostname": "204ac483-7e7e-4083-9ea2-c9ea22f459fd",
  "pid": 79465,
  "component": "HttpServer",
  "audit": true,
  "level": 30,
  "_audit": true,
  "operation": "getstorage",
  "billable_operation": "LIST",
  "logicalRemoteAddress": "172.27.4.22",
  "remoteAddress": "127.0.0.1",
  "remotePort": 64628,
  "reqHeaderLength": 754,
  "req": {
    "method": "GET",
    "url": "/poseidon/stor/manta_gc/mako/1.stor.staging.joyent.us?limit=1024",
    "headers": {
      "accept": "*/*",
      "x-request-id": "a080d88b-8e42-4a98-a6ec-12e1b0dbf612",
      "date": "Tue, 01 Aug 2017 03:03:13 GMT",
      "authorization": "Signature keyId=\"/poseidon/keys/ef:0e:27:45:c5:95:4e:92:ba:ab:03:17:e5:3a:60:14\",algorithm=\"rsa-sha256\",headers=\"date\",signature=\"Q74o9RHIwrDT15ogL2WeB/jankUIqJAtMM5t7+VzrHxzoB52/BoqEnq9uMY0wEvPJxv+Lf1VyLG/IBXCXeUx+fZlkhKWIWd2jkpLRdVLKwZ4nnqTfHM+YXhZ0vSN1X1W2demmnpPRTRK/RaG21pyvlbIrSTwI+N5MtKFDh9/4Ks43wSyM4MvqZZWywfs7LgKz7UtjL1Z+juhJDT8mrfQYCDpZw/NDhHmoslKsMFesMrMjPALy/CBSB23800+MhLiFB7LT0nTyCLonPBmIOjrQCZu99ICXbCxx096XCzZ2XBOK1Pe4eoDUHWx5ukTbCJV63QA+gvcvDCbS5BdDn0Xiw==\"",
      "user-agent": "restify/1.4.1 (ia32-sunos; v8/3.14.5.9; OpenSSL/1.0.1i) node/0.10.32",
      "accept-version": "~1.0",
      "host": "manta.staging.joyent.us",
      "connection": "keep-alive",
      "x-forwarded-for": "::ffff:172.27.4.22"
    },
    "httpVersion": "1.1",
    "owner": "4d649f41-cf87-ca1d-c2c0-bb6a9004311d",
    "caller": {
      "login": "poseidon",
      "uuid": "4d649f41-cf87-ca1d-c2c0-bb6a9004311d",
      "groups": [
        "operators"
      ],
      "user": null
    },
    "timers": {
      "earlySetup": 32,
      "parseDate": 8,
      "parseQueryString": 28,
      "handler-3": 127,
      "checkIfPresigned": 3,
      "enforceSSL": 3,
      "ensureDependencies": 5,
      "_authSetup": 5,
      "preSignedUrl": 3,
      "checkAuthzScheme": 4,
      "parseAuthTokenHandler": 36,
      "signatureHandler": 73,
      "parseKeyId": 59,
      "loadCaller": 133,
      "verifySignature": 483,
      "parseHttpAuthToken": 5,
      "loadOwner": 268,
      "getActiveRoles": 43,
      "gatherContext": 27,
      "setup": 225,
      "getMetadata": 5790,
      "storageContext": 8,
      "authorize": 157,
      "ensureEntryExists": 3,
      "assertMetadata": 3,
      "getDirectoryCount": 7903,
      "getDirectory": 10245
    }
  },
  "resHeaderLength": 269,
  "res": {
    "statusCode": 200,
    "headers": {
      "last-modified": "Sat, 22 Mar 2014 01:17:01 GMT",
      "content-type": "application/x-json-stream; type=directory",
      "result-set-size": 1,
      "date": "Tue, 01 Aug 2017 03:03:13 GMT",
      "server": "Manta",
      "x-request-id": "a080d88b-8e42-4a98-a6ec-12e1b0dbf612",
      "x-response-time": 26,
      "x-server-name": "204ac483-7e7e-4083-9ea2-c9ea22f459fd"
    }
  },
  "err": false,
  "latency": 26,
  "entryShard": "tcp://3.moray.staging.joyent.us:2020",
  "route": "getstorage",
  "msg": "handled: 200",
  "time": "2017-08-01T03:03:13.985Z",
  "v": 0
}
----


==== Contents of a PUT log entry

Here's an example log entry for a **PUT request**, formatted using the `json`
tool:

[source,json]
----
{
  "name": "muskie",
  "hostname": "204ac483-7e7e-4083-9ea2-c9ea22f459fd",
  "pid": 79465,
  "component": "HttpServer",
  "audit": true,
  "level": 30,
  "_audit": true,
  "operation": "putdirectory",
  "billable_operation": "PUT",
  "logicalRemoteAddress": "172.27.3.22",
  "reqHeaderLength": 655,
  "req": {
    "method": "PUT",
    "url": "/poseidon/stor/logs/config-agent/2017/08/01/02",
    "headers": {
      "user-agent": "curl/7.37.0",
      "host": "manta.staging.joyent.us",
      "accept": "*/*",
      "date": "Tue, 01 Aug 2017 03:01:10 GMT",
      "authorization": "Signature keyId=\"/poseidon/keys/ef:0e:27:45:c5:95:4e:92:ba:ab:03:17:e5:3a:60:14\",algorithm=\"rsa-sha256\",signature=\"VkRkcUK7Y796whM3/IsAl+wVvsu9pKwVGNHIHxLqeBtJZqrR+cbgWZ/E9uchhsxsMezvVXVN7hMXhiSxlfnGJKjPoTKJzfJNSW8WEUhu7rMilRi9WkYGvxo/PpdplK0/Evx1dvxHSX2TiAoTgBs5s6IyP7j6LgySfDu6TzJu/9HJdLzIwAf/TTiHU15okOUoJGbcNb+OcGN/mp+EZpYbNbJ8+I585v1ZLTuta1eAPngUPWp5E7Vm5sUpJH87/8bx2H/3HaMB9YCCacorZ7NkVS5Mbiaz0ptYYEESj8DCJScKnEVrM/L97zGuTPOnQ38Il/CZfENAP7ZH2u029h3WSg==\"",
      "connection": "close",
      "content-type": "application/json; type=directory",
      "x-forwarded-for": "::ffff:172.27.3.22"
    },
    "httpVersion": "1.1",
    "owner": "4d649f41-cf87-ca1d-c2c0-bb6a9004311d",
    "caller": {
      "login": "poseidon",
      "uuid": "4d649f41-cf87-ca1d-c2c0-bb6a9004311d",
      "groups": [
        "operators"
      ],
      "user": null
    },
    "timers": {
      "earlySetup": 94,
      "parseDate": 45,
      "parseQueryString": 32,
      "handler-3": 268,
      "checkIfPresigned": 8,
      "enforceSSL": 7,
      "ensureDependencies": 9,
      "_authSetup": 10,
      "preSignedUrl": 7,
      "checkAuthzScheme": 8,
      "parseAuthTokenHandler": 78,
      "signatureHandler": 155,
      "parseKeyId": 166,
      "loadCaller": 346,
      "verifySignature": 1164,
      "parseHttpAuthToken": 12,
      "loadOwner": 234,
      "getActiveRoles": 43,
      "gatherContext": 28,
      "setup": 315,
      "getMetadata": 13345,
      "storageContext": 14,
      "authorize": 409,
      "ensureParent": 222,
      "mkdir": 841
    }
  },
  "resHeaderLength": 215,
  "res": {
    "statusCode": 204,
    "headers": {
      "connection": "close",
      "last-modified": "Tue, 01 Aug 2017 03:01:01 GMT",
      "date": "Tue, 01 Aug 2017 03:01:11 GMT",
      "server": "Manta",
      "x-request-id": "ac2a5780-7665-11e7-b9e8-cf86a4bf1253",
      "x-response-time": 18,
      "x-server-name": "204ac483-7e7e-4083-9ea2-c9ea22f459fd"
    }
  },
  "latency": 18,
  "entryShard": "tcp://3.moray.staging.joyent.us:2020",
  "parentShard": "tcp://2.moray.staging.joyent.us:2020",
  "route": "putdirectory",
  "msg": "handled: 204",
  "time": "2017-08-01T03:01:11.048Z",
  "v": 0
}
----

==== Quick reference for Muskie log entry properties

Below is a summary of the most relevant fields for an audit log entry.  (Note
that Muskie sometimes writes out log entries unrelated to the completion of an
HTTP request.  Only log entries with `"audit": true` represent completion of an
HTTP request.  Other log entries have other fields.)

===== General Muskie-provided properties

[cols="3*",options="header"]
|===
|JSON property
|Example value
|Meaning

|`audit`
|`true`
|If `true`, this entry describes completion of an HTTP request.  Otherwise, this is some other type of log entry, and many of the fields below may not apply.

|`latency`
|26
|Time in milliseconds between when Muskie started processing this request and when the response _headers_ were sent.  This is commonly called _time to first byte_.  See also <<_build_a_request_timeline,building a request timeline>>.  This should generally match the `x-response-time` response header.

|`operation`
|`getstorage`
|Manta-defined token that describes the type of operation.  In this case, `getstorage` refers to an HTTP `GET` from a user's `stor` directory.

|`req`
|See specific properties below.
|Object describing the incoming request

|`req.method`
|`GET`
|HTTP method for this request (specified by the client)

|`req.url`
|`"/poseidon/stor/manta_gc/mako/1.stor.staging.joyent.us?limit=1024"`
|URL (path) provided for this request (specified by the client)

|`req.headers`
a|
[source,json]
----
{
    "accept": "*/*",
    "x-request-id": "a080d88b-8e42-4a98-a6ec-12e1b0dbf612",
    "date": "Tue, 01 Aug 2017 03:03:13 GMT",
    "authorization": "Signature keyId=\"/poseidon/keys/ef:0e:27:45:c5:95:4e:92:ba:ab:03:17:e5:3a:60:14\",algorithm=\"rsa-sha256\",headers=\"date\",signature=\"...\"",
    "user-agent": "restify/1.4.1 (ia32-sunos; v8/3.14.5.9; OpenSSL/1.0.1i) node/0.10.32",
    "accept-version": "~1.0",
    "host": "manta.staging.joyent.us",
    "connection": "keep-alive",
    "x-forwarded-for": "::ffff:172.27.4.22"
}
----

|Headers provided with this request (specified by the client).  The `Date` header is particularly useful to note, as this usually reflects the timestamp (on the client) when the client generated the request.  This is useful when <<_build_a_request_timeline,constructing a request timeline>>.  In particular, problems with the network (timeouts and retransmissions) or queueing any time before Muskie starts processing the request can be identified using this header, provided that the client clock is not too far off from the server clock.

|`req.caller`
a|
[source,json]
----
{
    "login": "poseidon",
    "uuid": "4d649f41-cf87-ca1d-c2c0-bb6a9004311d",
    "groups": [ "operators" ],
    "user": null
}
----

|Object describing the account making this request.  This is not the same as the owner!  Note that this can differ from the owner of the resource (`req.owner`).  That commonly happens when the caller uses operator privileges to access objects in someone else's account or when any user makes an authenticated request to access public data in some other user's account.

|`req.caller.login`
|`"poseidon"`
|For authenticated requests, the name of the account that made the request.

|`req.caller.uuid`
|`"4d649f41-cf87-ca1d-c2c0-bb6a9004311d"`
|For authenticated requests, the unique identifier for the account that made the request.

|`req.caller.groups`
|`[ "operators" ]`
|For authenticated requests, a list of groups that the caller is part of.  Generally, the only interesting group is `"operators"`, which grants the caller privileges to read from and write to any account.

|`req.caller.user`
|`null`
|For authenticated requests _from a subuser of the account_, the name of the subuser account.

|`req.owner`
|`"4d649f41-cf87-ca1d-c2c0-bb6a9004311d"`
|Unique identifier for the account that _owns_ the requested resource.  This is generally the uuid of the account at the start of the URL (i.e., for a request of `"/poseidon/stor"`, this would be the uuid of the account `poseidon`).

|`res`
|See specific properties below.
|Describes the HTTP response sent by Muskie to the client.

|`res.statusCode`
|200
|<<_http_status_codes_in_manta,HTTP-level status code>>.

|`res.headers`
a|
[source,json]
----
{
    "last-modified": "Sat, 22 Mar 2014 01:17:01 GMT",
    "content-type": "application/x-json-stream; type=directory",
    "result-set-size": 1,
    "date": "Tue, 01 Aug 2017 03:03:13 GMT",
    "server": "Manta",
    "x-request-id": "a080d88b-8e42-4a98-a6ec-12e1b0dbf612",
    "x-response-time": 26,
    "x-server-name": "204ac483-7e7e-4083-9ea2-c9ea22f459fd"
}
----

|Headers sent in the response from Muskie to the client.  Among the most useful is the `x-request-id` header, which should uniquely identify this request.  You can use this to correlate observations from the client or other parts of the system.

|`route`
|`"getstorage"`
|Identifies the name of the restify route that handled this request.

|===

===== Muskie-provided properties for debugging only

[cols="3*",options="header"]
|===
|JSON property
|Example value
|Meaning

|`entryShard`
|`"tcp://3.moray.staging.joyent.us:2020"`
|When present, this indicates the shard that was queried for the metadata for `req.url`.  Unfortunately, this field is not currently present when Muskie fails to fetch metadata, either because of a Moray failure or just because the metadata is missing (i.e., the path doesn't exist).

|`err`
|`false`
|Error associated with this request, if any.

|`objectId`
|`"bf54fb8a-6cb5-4683-8655-f9ad90b984d4"`
|When present, this is the unique identifier for the Manta object identified by `req.url` when the request was made.  This is helpful when trying to verify that a request fetched the exact object that you expect (and not another object that had the same name at the time).

|`parentShard`
|`"tcp://2.moray.staging.joyent.us:2020"`
|When present, this indicates the shard that was queried for the metadata for the parent directory of `req.url`.  This is only present when the parent metadata was fetched (which is common for PUT requests, but not GET or DELETE requests).  Unfortunately, this field is not currently present when Muskie fails to fetch metadata, either because of a Moray failure or just because the metadata is missing (i.e., the path doesn't exist).

|`logicalRemoteAddress`
|`"172.27.4.22"`
|The (remote) IP address of the client connected to Manta.  Note that clients aren't connected directly to Muskie.  When using TLS ("https" URLs), clients connect to `stud` in the `loadbalancer` component.  Stud connects to `haproxy` in the same container.  `haproxy` in the load balancer container connects to another `haproxy` instance in the Muskie container.  That `haproxy` instance connects to a Muskie process.  The client's IP is passed through this chain and recorded in `logicalRemoteAddress`.

|`remoteAddress`, `remotePort`
|`"127.0.0.1"`, `64628`
|The IP address and port of the TCP connection over which this request was received.  Generally, Muskie only connects directly to an `haproxy` inside the same zone, so the remote address will usually be `127.0.0.1`.  Neither of these fields is generally interesting except when debugging interactions with the local `haproxy`.

|`req.timers`
a|
[source,json]
----
{
    "earlySetup": 32,
    "parseDate": 8,
    "parseQueryString": 28,
    "handler-3": 127,
    "checkIfPresigned": 3,
    "enforceSSL": 3,
    "ensureDependencies": 5,
    "_authSetup": 5,
    "preSignedUrl": 3,
    "checkAuthzScheme": 4,
    "parseAuthTokenHandler": 36,
    "signatureHandler": 73,
    "parseKeyId": 59,
    "loadCaller": 133,
    "verifySignature": 483,
    "parseHttpAuthToken": 5,
    "loadOwner": 268,
    "getActiveRoles": 43,
    "gatherContext": 27,
    "setup": 225,
    "getMetadata": 5790,
    "storageContext": 8,
    "authorize": 157,
    "ensureEntryExists": 3,
    "assertMetadata": 3,
    "getDirectoryCount": 7903,
    "getDirectory": 10245
}
----
|An object describing the time in microseconds for each phase of the request processing pipeline.  This is useful for identifying latency.  The names in this object are the names of functions inside Muskie responsible for the corresponding phase of request processing.

|`sharksContacted`
a|
[source,json]
----
[ {
  "shark": "1.stor.staging.joyent.us",
  "result": "ok",
  "timeToFirstByte": 2,
  "timeTotal": 902,
  "_startTime": 1509505866032
}, {
  "shark": "2.stor.staging.joyent.us",
  "result": "ok",
  "timeToFirstByte": 1,
  "timeTotal": 870,
  "_startTime": 1509505866033
} ]
----
a|This field should be present for Manta requests that make requests to individual storage nodes.  The value is an array of storage nodes contacted as part of the request, including the result of this subrequest, when it started, and how long it took.

For GET requests, these subrequests are GET requests from individual storage nodes hosting a copy of the object requested.  These subrequests happen serially, and we stop as soon as one completes.

For PUT requests, the storage node subrequests are PUT requests to individual storage nodes on which a copy of the new object will be stored.  If all goes well, you'll see N sharks contacted (typically 2, but whatever the client's requested durability level is), all successfully, and the requests will be concurrent with each other.  If any of these fail, Manta will try another N sharks, and up to one more set of N.  For durability level 2, you may see up to 6 sharks contacted: three sets of two.  The sets would be sequential, while each pair in a set run concurrently.



|===


===== https://github.com/trentm/node-bunyan#core-fields[Bunyan]-provided properties

[cols="3*",options="header"]
|===
|JSON property
|Example value
|Meaning

|`time`
|`"2017-08-01T03:03:13.985Z"`
|ISO 8601 timestamp closest to when the log entry was generated.  

|`hostname`
|`"204ac483-7e7e-4083-9ea2-c9ea22f459fd"`
|The hostname of the system that generated the log entry.  For us, this is generally a uuid corresponding to the zonename of the Muskie container.

|`pid`
|`79465`
|The pid of the process that generated the log entry.

|`level`
|`30`
|Bunyan-defined log level.  This is a numeric value corresponding to conventional values like `'debug'`, `'info'`, `'warn'`, etc.  You can filter based on level using the `bunyan` command.

|`msg`
|`"handled: 200"`
|For Muskie audit log entries, the message is always `"handled: "` followed by the HTTP level status code.
|===




XXX talk about common stack traces?
XXX that should include 503 from 'No storage nodes available for this request'

=== Understanding latency for a specific request

=== Finding a load balancer log entry

==== When to investigate the load balancer

All HTTP requests to Manta travel through an haproxy-based load balancer (in a
component sometimes called "muppet") before reaching the Manta API ("webapi" or
"muskie").  This load balancer is one of the first components that processes
these requests when they arrive at Manta.  For many problems internal to Manta,
it's more useful to look at <<_understanding_a_muskie_log_entry,log entries at
Muskie (webapi)>>.  However, there are a several cases where it's helpful to investigate the load balancer:

- when investigating a problem where the client reports _not_ having received a
  normal HTTP response (e.g., a "connection refused", "connection reset", or a
  connection timeout)
- when investigating a problem where the client reports having received a
  500-level error with no "x-server-name" header.  (This generally indicates the
  response was sent by the load balancer, which happens when Muskie sends an
  invalid response or fails to send a response within a given timeout.)
- when investigating a problem where Muskie reports surprising client behavior
  (e.g., client closed its connection mid-upload, or Muskie timed out waiting
  for a client to either upload or download data)
- when investigating a failed request for which there appears to be no Muskie
  log entry at all
- to identify the source IP address of a client in a case where Muskie fails to
  report that in its own log entry

Generally, if a client receives a well-formed response from Manta, the Muskie
logs have more useful information than the load balancer logs.  In these other
cases where either the client or Muskie is doing something surprising, the load
balancer log entries can provide more information about exactly what happened.

There's a major caveat to the load balancer logs: **haproxy is only able to
interpret HTTP-level information about the first request on each TCP
connection.**  For clients using HTTP keep-alive, where multiple HTTP requests
are sent sequentially over a TCP connection, you may not find information in the
haproxy logs about requests after the first one.

==== Finding load balancer log entries

First, see <<_locating_log_files>> for information on where to find real-time
and historical log files.  For the load balancer, real-time log files are stored
in `/var/log/haproxy.log`.  Historical log files are stored in Manta under
`/poseidon/stor/logs/haproxy`.

Usually when you're looking for a load balancer log entry, you know one or more
of the following:

- an approximate time of the request (almost always necessary)
- the URL that was requested
- the remote IP address that made the request
- the status code that was returned
- that the request experienced an unusual HTTP exchange (e.g., a malformed
  server response, a client timeout, or a server timeout)
- the particular load balancer that handled the request

Since these are plaintext logs, you can use `grep` or `awk` to filter or
summarize them.  See <<_understanding_a_load_balancer_log_entry>> for more.

Often you don't know which load balancer handled a particular request.  In
that case, you need to scan all of them for a given time.  That might involve
`manta-oneach` or a compute job.  Again, see <<_locating_log_files>>.

=== Understanding a load balancer log entry

The load balancer logs in a plaintext format described in the
https://github.com/joyent/haproxy-1.5/blob/master/doc/haproxy-en.txt[haproxy
documentation].  (The haproxy documentation is also plaintext so it's not
possible to link directly to the right section, but look for the section called
"Log format".)  Our load balancer logs these through syslog, which prepends a
few fields.

Here's an example entry from our load balancer logs:


    2018-12-05T18:32:01+00:00 42563f8d-4d61-4045-ab87-c71560388399 haproxy[65158]: ::ffff:72.2.115.97:42121 [05/Dec/2018:18:30:01.859] https secure_api/be6 2/0/0/-1/120005 502 245 - - SH-- 247/192/240/19/0 0/0 "GET /thoth/stor/thoth?limit=1024 HTTP/1.1"

We have a tool called https://github.com/joyent/node-haproxy-log[haplog] for
converting our haproxy log entries into JSON.  Often, the easiest way to filter
and summarize these log entries is to pass the log through `haplog`, use `json`
to extract the relevant fields, and then use `grep` or `awk` to summarize.  We
can use it like this:

    $ haplog 42563f8d.log | json
    ...
    {
      "syslog_date": "2018-12-05T18:32:01.000Z",
      "syslog_hostname": "42563f8d-4d61-4045-ab87-c71560388399",
      "pid": 65158,
      "client_ip": "72.2.115.97",
      "client_port": 42121,
      "accept_date": "05/Dec/2018:18:30:01.859",
      "frontend_name": "https",
      "backend_name": "secure_api",
      "server_name": "be6",
      "Tq": 2,
      "Tw": 0,
      "Tc": 0,
      "Tr": -1,
      "Tt": 120005,
      "status_code": 502,
      "bytes_read": 245,
      "termination_state": {
        "raw": "SH--",
        "termination_cause": "BACKEND_ABORT",
        "state_at_close": "WAITING_FOR_RESPONSE_HEADERS",
        "persistence_cookie_client": "N/A",
        "persistence_cookie_server": "N/A"
      },
      "actconn": 247,
      "feconn": 192,
      "beconn": 240,
      "srv_conn": 19,
      "retries": 0,
      "srv_queue": 0,
      "backend_queue": 0,
      "http_request": "GET /thoth/stor/thoth?limit=1024 HTTP/1.1"
    }

There's quite a lot of information here!  Among the most relevant bits:

[cols="3*",options="header"]
|===
|Example value
|haplog JSON field name
|Meaning

|`"2018-12-05T18:32:01.000Z"`
|`syslog_date`
|The timestamp when this entry was logged.  This usually corresponds to the end
of an HTTP request or TCP connection.  This is very useful for constructing
timelines of what happened.

|`"42563f8d-4d61-4045-ab87-c71560388399"`
|`syslog_hostname`
|On our systems, this is the zonename of the particular load balancer that
handled this request.

|`"72.2.115.97"`
| `client_ip`
| The (remote) IP address of the client that connected to Manta.

|`"05/Dec/2018:18:30:01.859"`
|`accept_date`
|The timestamp when the TCP connection was accepted by the load balancer.  This
is very useful for constructing timelines of what happened.

|`"be6"`
|`server_name`
|On our system, this is a unique identifier that indicates which Muskie zone
handled this request.  This identifier varies between load balancer zones and
over time.  In order to know which Muskie zone this corresponds to, you need to
find the corresponding line in the haproxy log file (at
`/opt/smartdc/muppet/etc/haproxy.cfg`).  This will contain the IP address of the
Muskie zone that handled the request.  This is useful for understanding the flow
of the request through Manta.

|`"SH--"`
|`termination_state`
|This is a four-digit code that describes how the TCP session finally
terminated.  **This is among the most useful fields for understanding abnormal
behavior from the client or Muskie.**  This code can be used to tell whether the
client or server either did something unexpected (like closed the TCP
connection) or stopped responding.  For details, on what each code means, see
the haproxy documentation linked above.

|`"GET /thoth/stor/thoth?limit=1024 HTTP/1.1"`
|`http_request`
|The first line of the HTTP request, which contains the HTTP method and request
URL.
|===

There are a few telltale symptoms here:

- The elapsed time between `accept_date` and `syslog_date` is exactly two
  minutes.  The load balancer has a two-minute timeout for Muskie responding to
  requests.
- The termination status `SH--` is documented (in the haproxy docs) to mean:
+
> SH    The server aborted before sending its full headers, or it crashed.
  
This entry appears to reflect Muskie closing its TCP connection (without sending
an HTTP response) after exactly two minutes.  If we didn't know what happened
here, we at least know now that Muskie did something unexpected and not the
client, and we also know which Muskie zone it was.

=== Build a request timeline

// XXX
// loadbalancer log: accept time, completion time
// muskie log:
// - completion time
// - client "Date" header (note: different clock)
// - latency (only for time-to-first-byte)
// - request timers
// - shark information
// object metadata (e.g., mtime on record)
// mako access log entries

=== Details about specific error messages

==== `No storage nodes available for this request`

==== `Not enough free space for ... MB`

This code (associated with 507 errors) indicates that Manta does not have enough
space available on any storage nodes for the write _that was requested_.  This
would be surprising in production environments, although it's easy to induce
even in production by requesting an absurd size.  For example, you'll see this
if you attempt to upload an enormous object:

[source,text]
----
$ mput -H 'max-content-length: 1125899906842624' /dap/stor/waytoobig
mput: NotEnoughSpaceError: not enough free space for 1073741824 MB
----

https://apidocs.joyent.com/manta/api.html#PutObject[Recall that Manta supports
two kinds of uploads]: streaming and fixed-length.

Streaming uploads are specified using the `transfer-encoding: chunked` header.
In this case, the space allocated up front (and validated) is specified by the
`max-content-length` header.  If that header is missing, a default value is used
_that is sometimes too large in some development environments_, which can cause
streaming uploads to fail in development environments.

If an object PUT does not specify `transfer-encoding: chunked`, then it must
specify `content-length`, in which case that value is used for allocation (and
validation).

=== Locating metadata for an object

See http://joyent.github.io/manta/#locating-object-data[Locating object data] in
the Manta Operator's Guide.

=== Locating actual data for an object

See http://joyent.github.io/manta/#locating-object-data[Locating object data] in
the Manta Operator's Guide.

=== Locating a particular server

See http://joyent.github.io/manta/#locating-servers[Locating servers] in the
Manta Operator's Guide.

=== Locating a particular zone

See http://joyent.github.io/manta/#locating-manta-component-zones[Locating Manta
component zones] in the Manta Operator's Guide.

=== Locating a particular database shard


=== Finding what shard a particular zone is part of


=== Save debugging state and restart a process

// TODO include filing a bug

=== Investigating service discovery

// TODO what's in DNS

=== Investigating a slow process

// TODO this should probably just be: check if it's on-CPU and branch to one of
// the following steps

=== Investigating why a process is on-CPU

// TODO profiling a busy process

=== Investigating why a process is off-CPU

=== Check for a garbage collection issue (or memory leak)

//    - Check if the process is on-CPU
//    - Checking if a process is doing GC
//    - Check the vsz/rss
//    - Gcore and restart
//    - Watch for a few minutes.  Is it better?
//      Yes: check other instances for vsz/rss and restart them
//      No: maybe leak is too fast?
//      - Was there a recent change that may have introduced this?
//        Yes: rollback
//        No: debug memory leak


=== Scaling up a component

// TODO: caveats for webapi (will restart load balancers), moray (will want to
// tune *down* database connections, or be aware that you'll be using more)


=== Characterizing a problem

It's very valuable to _briefly_ and _precisely_ characterize a problem:

- when asking for help from others, so they can quickly understand the relevant
  context
- for summarizing status to stakeholders
- for handing off work to others (e.g., when escalating to another team or
  changing shifts)
- as a form of
  https://en.wikipedia.org/wiki/Rubber_duck_debugging[rubber-ducking] -- it
  often helps spark insight

When characterizing a problem, include the basic facts:

- Exactly **what** you've observed: 500 errors? 503 errors? Elevated latency?
  It's useful to be as specific as you can (e.g., "a 5% increase in 500
  errors"), but it's better to be vague (e.g., "an increase in latency") than to
  say something false (e.g., "an increase in average latency" when only tail
  latency is affected).
- Something about **when** the observations started.  Again, it's useful to be
  as specific as possible (e.g., "starting at 2018-08-09T16:47Z"), but it's
  better to be vague (e.g., "seems to have increased since yesterday") than
  incorrect (e.g., "it's higher than yesterday" when you're really just
  eyeballing an average value from a graph).
- What other observations you've made (or anything else you've tried, and the
  results)
- (if you're asking for help) Any specific questions you have.

It's useful to mention what conclusions you've drawn, but try to distinguish
facts (e.g., "moray queue lengths are high") from inferences ("moray is
overloaded").

Here's a good status summary:

[quote]
Webapi is reporting an increase in 500 and 503 errors since 2018-08-09T16:58Z.
These do not seem correlated with increased latency, nor with any errors from
the metadata tier.  How do we check for errors from the storage tier?

It's often very helpful to include screenshots
(https://support.apple.com/en-us/HT201361[MacOS instructions]).  Here's an
example:

image::images/metrics-webapi-elevated-1.png[Elevated latency at Webapi]

Here's one way to characterize this:

[quote]
We saw elevated latency since early on 8/1 (UTC)

Better:

[quote]
We saw a modest increase in tail latency (both p90 and p99) starting early on
8/1 and continuing through 8/4 (UTC).  This was followed by a sharp, significant
increase on 8/6.

More detailed (may be better, depending on the situation):

[quote]
We saw a modest increase in tail latency (both p90 and p99) starting early on
8/1 and continuing through 8/4 (UTC).  This was followed by a sharp, significant
increase on 8/6.  During both increases, latency was much less stable than
before.  p90 peaked at about 50% higher than normal, while p95 spiked about 3x
what it was before.

Of course, it's easy to give too much detail, too.  Think about what's likely
to be relevant for your audience.

**Grafana's features can help you make quantitative statements.**  For example,
take this graph:

image::images/metrics-webapi-bytes-1.png[Dips in throughput at Webapi]

About the dip on 8/6, you could say:

[quote]
We saw a dip in throughput on 8/6.

Better would be:

[quote]
There was a brief dip in inbound throughput on 8/6 around 12:36Z.

Since Grafana can show you both the average for the whole period, plus the value
at a point, you can make more specific statements:

[quote]
There was a brief dip in inbound throughput on 8/6 around 12:36Z.  It dropped
about 38% relative to the average for 14 days around it (3.3 GBps during the
dip, compared to a recent average of about 5.3 GBps).
